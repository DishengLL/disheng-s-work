{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaeeeb17",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#skip-gram-model\" data-toc-modified-id=\"skip-gram-model-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>skip-gram model</a></span><ul class=\"toc-item\"><li><span><a href=\"#corpus\" data-toc-modified-id=\"corpus-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>corpus</a></span></li><li><span><a href=\"#preprocessing\" data-toc-modified-id=\"preprocessing-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>preprocessing</a></span></li></ul></li><li><span><a href=\"#PyTorch-Word2Vec-Network\" data-toc-modified-id=\"PyTorch-Word2Vec-Network-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>PyTorch Word2Vec Network</a></span><ul class=\"toc-item\"><li><span><a href=\"#preprocessing\" data-toc-modified-id=\"preprocessing-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>preprocessing</a></span></li><li><span><a href=\"#establish-network\" data-toc-modified-id=\"establish-network-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>establish network</a></span></li><li><span><a href=\"#save-model\" data-toc-modified-id=\"save-model-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>save model</a></span></li><li><span><a href=\"#extract-word-embedding\" data-toc-modified-id=\"extract-word-embedding-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>extract word embedding</a></span></li><li><span><a href=\"#Visualizaion\" data-toc-modified-id=\"Visualizaion-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Visualizaion</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d83da8",
   "metadata": {},
   "source": [
    "# skip-gram model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a7407c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.functional as F\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ee88b4",
   "metadata": {},
   "source": [
    "## corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99355bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'he is a king',\n",
    "    'she is a queen',\n",
    "    'he is a man',\n",
    "    'she is a woman',\n",
    "    'queen is a woman',\n",
    "    'king is a man',\n",
    "    'warsaw is poland capital',\n",
    "    'berlin is germany capital',\n",
    "    'paris is france capital',   \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025a5a23",
   "metadata": {},
   "source": [
    "## preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "581dcc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    tokens = [x.split() for x in corpus]\n",
    "    return tokens\n",
    "\n",
    "tokenized_corpus = tokenize_corpus(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ef2810d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['he', 'is', 'a', 'king'],\n",
       " ['she', 'is', 'a', 'queen'],\n",
       " ['he', 'is', 'a', 'man'],\n",
       " ['she', 'is', 'a', 'woman'],\n",
       " ['queen', 'is', 'a', 'woman'],\n",
       " ['king', 'is', 'a', 'man'],\n",
       " ['warsaw', 'is', 'poland', 'capital'],\n",
       " ['berlin', 'is', 'germany', 'capital'],\n",
       " ['paris', 'is', 'france', 'capital']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a03b3f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = []\n",
    "for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "vocabulary_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85291ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he',\n",
       " 'is',\n",
       " 'a',\n",
       " 'king',\n",
       " 'she',\n",
       " 'queen',\n",
       " 'man',\n",
       " 'woman',\n",
       " 'warsaw',\n",
       " 'poland',\n",
       " 'capital',\n",
       " 'berlin',\n",
       " 'germany',\n",
       " 'paris',\n",
       " 'france']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02de28cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "idx_pairs = []\n",
    "# for each sentence\n",
    "for sentence in tokenized_corpus:\n",
    "    indices = [word2idx[word] for word in sentence]\n",
    "    # for each word, threated as center word\n",
    "    for center_word_pos in range(len(indices)):\n",
    "        # for each window position\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_word_pos = center_word_pos + w\n",
    "            # make soure not jump out sentence\n",
    "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                continue\n",
    "            context_word_idx = indices[context_word_pos]\n",
    "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array\n",
    "# idx_pairs-----(center_word, context_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "750a521e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1],\n",
       "       [ 0,  2],\n",
       "       [ 1,  0],\n",
       "       [ 1,  2],\n",
       "       [ 1,  3],\n",
       "       [ 2,  0],\n",
       "       [ 2,  1],\n",
       "       [ 2,  3],\n",
       "       [ 3,  1],\n",
       "       [ 3,  2],\n",
       "       [ 4,  1],\n",
       "       [ 4,  2],\n",
       "       [ 1,  4],\n",
       "       [ 1,  2],\n",
       "       [ 1,  5],\n",
       "       [ 2,  4],\n",
       "       [ 2,  1],\n",
       "       [ 2,  5],\n",
       "       [ 5,  1],\n",
       "       [ 5,  2],\n",
       "       [ 0,  1],\n",
       "       [ 0,  2],\n",
       "       [ 1,  0],\n",
       "       [ 1,  2],\n",
       "       [ 1,  6],\n",
       "       [ 2,  0],\n",
       "       [ 2,  1],\n",
       "       [ 2,  6],\n",
       "       [ 6,  1],\n",
       "       [ 6,  2],\n",
       "       [ 4,  1],\n",
       "       [ 4,  2],\n",
       "       [ 1,  4],\n",
       "       [ 1,  2],\n",
       "       [ 1,  7],\n",
       "       [ 2,  4],\n",
       "       [ 2,  1],\n",
       "       [ 2,  7],\n",
       "       [ 7,  1],\n",
       "       [ 7,  2],\n",
       "       [ 5,  1],\n",
       "       [ 5,  2],\n",
       "       [ 1,  5],\n",
       "       [ 1,  2],\n",
       "       [ 1,  7],\n",
       "       [ 2,  5],\n",
       "       [ 2,  1],\n",
       "       [ 2,  7],\n",
       "       [ 7,  1],\n",
       "       [ 7,  2],\n",
       "       [ 3,  1],\n",
       "       [ 3,  2],\n",
       "       [ 1,  3],\n",
       "       [ 1,  2],\n",
       "       [ 1,  6],\n",
       "       [ 2,  3],\n",
       "       [ 2,  1],\n",
       "       [ 2,  6],\n",
       "       [ 6,  1],\n",
       "       [ 6,  2],\n",
       "       [ 8,  1],\n",
       "       [ 8,  9],\n",
       "       [ 1,  8],\n",
       "       [ 1,  9],\n",
       "       [ 1, 10],\n",
       "       [ 9,  8],\n",
       "       [ 9,  1],\n",
       "       [ 9, 10],\n",
       "       [10,  1],\n",
       "       [10,  9],\n",
       "       [11,  1],\n",
       "       [11, 12],\n",
       "       [ 1, 11],\n",
       "       [ 1, 12],\n",
       "       [ 1, 10],\n",
       "       [12, 11],\n",
       "       [12,  1],\n",
       "       [12, 10],\n",
       "       [10,  1],\n",
       "       [10, 12],\n",
       "       [13,  1],\n",
       "       [13, 14],\n",
       "       [ 1, 13],\n",
       "       [ 1, 14],\n",
       "       [ 1, 10],\n",
       "       [14, 13],\n",
       "       [14,  1],\n",
       "       [14, 10],\n",
       "       [10,  1],\n",
       "       [10, 14]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da5b5e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(vocabulary_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63efd587",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epo 0: 5.365805343786875\n",
      "Loss at epo 10: 4.265196937074264\n",
      "Loss at epo 20: 3.7574497762653563\n",
      "Loss at epo 30: 3.427095933755239\n",
      "Loss at epo 40: 3.183838502400451\n",
      "Loss at epo 50: 2.992399157087008\n",
      "Loss at epo 60: 2.835820076531834\n",
      "Loss at epo 70: 2.7055015221238135\n",
      "Loss at epo 80: 2.596596153742737\n",
      "Loss at epo 90: 2.505732663638062\n",
      "Loss at epo 100: 2.429963717692428\n"
     ]
    }
   ],
   "source": [
    "embedding_dims = 5\n",
    "W1 = Variable(torch.randn(embedding_dims, vocabulary_size).float(), requires_grad=True)\n",
    "W2 = Variable(torch.randn(vocabulary_size, embedding_dims).float(), requires_grad=True)\n",
    "num_epochs = 101\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for data, target in idx_pairs:\n",
    "        x = Variable(get_input_layer(data)).float()\n",
    "        y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
    "\n",
    "        z1 = torch.matmul(W1, x)\n",
    "        z2 = torch.matmul(W2, z1)\n",
    "\n",
    "\n",
    "    \n",
    "        log_softmax = F.log_softmax(z2, dim=0)\n",
    "#         print(log_softmax.view(1,-1).shape)\n",
    "#         print(log_softmax.shape)\n",
    "#         print(y_true)\n",
    "        \n",
    "\n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "\n",
    "        loss_val += loss.item()\n",
    "        loss.backward()\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "    if epo % 10 == 0:    \n",
    "        print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe5458dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 15])\n",
      "torch.Size([5, 15])\n",
      "torch.Size([5, 15])\n"
     ]
    }
   ],
   "source": [
    "print(W1.shape)\n",
    "print(W2.T.shape)\n",
    "W = W1+W2.T\n",
    "print((W/2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b83014c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aae00a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b61d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b791077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad516360",
   "metadata": {},
   "source": [
    "# PyTorch Word2Vec Network\n",
    "\n",
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1543291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'he is a king',\n",
    "    'she is a queen',\n",
    "    'he is a man',\n",
    "    'she is a woman',\n",
    "    'warsaw is poland capital',\n",
    "    'berlin is germany capital',\n",
    "    'paris is france capital', \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc1ea92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    tokens = [x.split() for x in corpus]\n",
    "    return tokens\n",
    "\n",
    "tokenized_corpus = tokenize_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2cf969cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = []\n",
    "for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49eaa04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "idx_pairs = []\n",
    "# for each sentence\n",
    "for sentence in tokenized_corpus:\n",
    "    indices = [word2idx[word] for word in sentence]\n",
    "    # for each word, threated as center word\n",
    "    for center_word_pos in range(len(indices)):\n",
    "        # for each window position\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_word_pos = center_word_pos + w\n",
    "            # make soure not jump out sentence\n",
    "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                continue\n",
    "            context_word_idx = indices[context_word_pos]\n",
    "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6abc3f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(vocabulary_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7efc1d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'he': 0,\n",
       " 'is': 1,\n",
       " 'a': 2,\n",
       " 'king': 3,\n",
       " 'she': 4,\n",
       " 'queen': 5,\n",
       " 'man': 6,\n",
       " 'woman': 7,\n",
       " 'warsaw': 8,\n",
       " 'poland': 9,\n",
       " 'capital': 10,\n",
       " 'berlin': 11,\n",
       " 'germany': 12,\n",
       " 'paris': 13,\n",
       " 'france': 14}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b89cd0e",
   "metadata": {},
   "source": [
    "## establish network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e8c08a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.functional as F\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8ebf9b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (linear_w2v): Sequential(\n",
      "    (0): Linear(in_features=15, out_features=5, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=5, out_features=15, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "dim = 5\n",
    "vocabulary_size = vocabulary_size\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self,dim,vocabulary_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.vocabulary_size=vocabulary_size\n",
    "        self.linear_w2v = nn.Sequential(\n",
    "            nn.Linear(self.vocabulary_size , self.dim),\n",
    "            nn.Sigmoid(),\n",
    "            #nn.ReLU(),\n",
    "            nn.Linear(self.dim, self.vocabulary_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = self.flatten(x)\n",
    "        res = self.linear_w2v(x)\n",
    "        #res = nn.Softmax(dim=0)(res)\n",
    "        res = F.log_softmax(res, dim=0)\n",
    "        return res\n",
    "\n",
    "model = NeuralNetwork(5,vocabulary_size = vocabulary_size).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "650ee11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=3e-3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e239551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(idx_pairs, model, loss_fn, optimizer,num_epochs):\n",
    "    model.train()\n",
    "    for epo in range(num_epochs):\n",
    "        for data, target in idx_pairs:\n",
    "            x = get_input_layer(data)\n",
    "\n",
    "            y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
    "            \n",
    "            x, y_true = x.to(device), y_true.to(device)\n",
    "#             print(y_true.shape)\n",
    "            \n",
    "            pred = model(x)\n",
    "#             print(pred.view(1,-1).shape)\n",
    "            loss = loss_fn(pred.view(1,-1), y_true)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if epo % 1000 == 0:    \n",
    "            print( f\"loss: {loss:>7f}  [{epo:>5d}/{num_epochs:>5d}]\")\n",
    "       \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "107b38a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.643618  [    0/10000]\n",
      "loss: 2.806059  [ 1000/10000]\n",
      "loss: 2.074090  [ 2000/10000]\n",
      "loss: 1.897772  [ 3000/10000]\n",
      "loss: 1.829331  [ 4000/10000]\n",
      "loss: 1.832941  [ 5000/10000]\n",
      "loss: 1.870591  [ 6000/10000]\n",
      "loss: 1.901261  [ 7000/10000]\n",
      "loss: 1.903641  [ 8000/10000]\n",
      "loss: 1.885684  [ 9000/10000]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10000\n",
    "train(idx_pairs, model, loss_fn, optimizer,num_epochs)\n",
    "# test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b43f3a",
   "metadata": {},
   "source": [
    "## save model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b4f94e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (linear_w2v): Sequential(\n",
       "    (0): Linear(in_features=15, out_features=5, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=5, out_features=15, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"w2v_model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3dec017c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for NeuralNetwork:\n\tsize mismatch for linear_w2v.0.weight: copying a param with shape torch.Size([5, 15]) from checkpoint, the shape in current model is torch.Size([8, 15]).\n\tsize mismatch for linear_w2v.0.bias: copying a param with shape torch.Size([5]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for linear_w2v.2.weight: copying a param with shape torch.Size([15, 5]) from checkpoint, the shape in current model is torch.Size([15, 8]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pp/qtk1lyns6zg8x9dlj6f5xvtm0000gn/T/ipykernel_40396/3638110870.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"w2v_model.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/python-run/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1483\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1484\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for NeuralNetwork:\n\tsize mismatch for linear_w2v.0.weight: copying a param with shape torch.Size([5, 15]) from checkpoint, the shape in current model is torch.Size([8, 15]).\n\tsize mismatch for linear_w2v.0.bias: copying a param with shape torch.Size([5]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for linear_w2v.2.weight: copying a param with shape torch.Size([15, 5]) from checkpoint, the shape in current model is torch.Size([15, 8])."
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(dim,vocabulary_size)\n",
    "model.load_state_dict(torch.load(\"w2v_model.pth\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31609b2b",
   "metadata": {},
   "source": [
    "## extract word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "db13fbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_w2v.0.weight\n",
      "--\n",
      "Parameter containing:\n",
      "tensor([[ 1.2015e+00, -1.0389e+00, -2.3440e+00,  1.1638e+00,  1.2037e+00,\n",
      "          1.1665e+00,  1.1683e+00,  1.1705e+00, -2.3201e+00,  2.1557e+00,\n",
      "         -2.0076e+00, -1.4280e+00,  2.9285e-01, -2.3417e+00,  4.4944e+00],\n",
      "        [ 1.2386e-01, -9.1122e-02, -2.4335e-01, -2.4151e-01, -9.0420e-02,\n",
      "          8.9497e-02,  2.1107e-01, -5.8691e-02, -1.4327e-01, -1.6976e-01,\n",
      "         -2.5615e-01,  2.4340e-02, -1.8688e-01, -2.2939e-01,  2.4763e-02],\n",
      "        [-2.2950e-01,  2.5879e+00, -2.3201e-01, -2.2780e-01, -2.3282e-01,\n",
      "         -2.2865e-01, -2.2926e-01, -2.3299e-01, -2.0410e-01, -2.2839e-01,\n",
      "         -2.3066e-01, -2.2990e-01, -2.2427e-01, -2.2456e-01, -2.2040e-01],\n",
      "        [-9.1927e-02,  1.4393e-01, -7.0724e-03,  1.7079e-02, -1.4363e-01,\n",
      "          8.6440e-02,  3.0028e-02,  4.8143e-02,  3.4159e-03, -1.1509e-01,\n",
      "         -2.9805e-02, -1.8536e-01,  1.4871e-01,  7.1334e-02, -6.7604e-02],\n",
      "        [-9.0249e-01,  8.2928e-02,  1.4584e+00, -9.2534e-01, -9.0258e-01,\n",
      "         -9.2565e-01, -9.2581e-01, -9.2556e-01, -1.1308e+00,  3.9888e+00,\n",
      "         -1.3165e+00, -1.3168e+00,  4.4946e+00, -6.3458e-01,  2.3323e+00]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "linear_w2v.0.bias\n",
      "--\n",
      "Parameter containing:\n",
      "tensor([ 2.3430, -0.2116,  0.0854, -0.1490,  1.3148], requires_grad=True)\n",
      "\n",
      "\n",
      "linear_w2v.2.weight\n",
      "--\n",
      "Parameter containing:\n",
      "tensor([[-1.7780,  0.1194,  0.6769,  0.2329,  1.6627],\n",
      "        [ 1.7470, -0.0727, -3.9719, -0.1499, -0.0623],\n",
      "        [ 3.6361, -0.3563,  0.7845, -0.4388, -3.0767],\n",
      "        [-1.3438, -0.3703,  0.3640, -0.2841,  1.4439],\n",
      "        [-1.9925,  0.2283,  0.7825, -0.1264,  1.6756],\n",
      "        [-1.5124, -0.0805,  0.4560,  0.3461,  1.4719],\n",
      "        [-1.6307, -0.3688,  0.5288, -0.1541,  1.5078],\n",
      "        [-1.7490, -0.1961,  0.5607, -0.1878,  1.4633],\n",
      "        [ 2.1719, -0.3654, -0.0486, -0.2041,  0.9866],\n",
      "        [-2.5520, -0.3401,  1.1523, -0.2239, -3.2505],\n",
      "        [ 2.3188,  0.4123,  0.1307, -0.2550,  1.0128],\n",
      "        [ 0.8580,  0.0138,  0.3201, -0.2771,  1.8326],\n",
      "        [ 1.1401, -0.3017,  0.0150, -0.3613, -3.6667],\n",
      "        [ 3.0484,  0.3938,  0.3229, -0.2617, -0.0827],\n",
      "        [-1.5651, -0.3683, -0.0620,  0.2785, -1.4245]], requires_grad=True)\n",
      "\n",
      "\n",
      "linear_w2v.2.bias\n",
      "--\n",
      "Parameter containing:\n",
      "tensor([-0.6200,  4.8660, -0.5341, -0.7144, -0.6311, -0.7763, -0.8623, -0.7257,\n",
      "        -3.4742,  5.3231, -3.0382, -3.9516,  4.2160, -4.0864,  4.7841],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print('--')\n",
    "    print(param)\n",
    "    print('\\n')\n",
    "    if 'weight' in name :\n",
    "        res.append(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5a279069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2883,  0.1216,  0.2237,  0.0705,  0.3801],\n",
       "        [ 0.3541, -0.0819, -0.6920, -0.0030,  0.0103],\n",
       "        [ 0.6461, -0.2998,  0.2763, -0.2229, -0.8091],\n",
       "        [-0.0900, -0.3059,  0.0681, -0.1335,  0.2593],\n",
       "        [-0.3944,  0.0689,  0.2748, -0.1350,  0.3865],\n",
       "        [-0.1730,  0.0045,  0.1137,  0.2163,  0.2731],\n",
       "        [-0.2312, -0.0789,  0.1498, -0.0620,  0.2910],\n",
       "        [-0.2893, -0.1274,  0.1639, -0.0698,  0.2689],\n",
       "        [-0.0741, -0.2543, -0.1263, -0.1003, -0.0721],\n",
       "        [-0.1981, -0.2549,  0.4620, -0.1695,  0.3691],\n",
       "        [ 0.1556,  0.0781, -0.0500, -0.1424, -0.1518],\n",
       "        [-0.2850,  0.0191,  0.0451, -0.2312,  0.2579],\n",
       "        [ 0.7165, -0.2443, -0.1047, -0.1063,  0.4139],\n",
       "        [ 0.3533,  0.0822,  0.0492, -0.0952, -0.3586],\n",
       "        [ 1.4647, -0.1718, -0.1412,  0.1055,  0.4539]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ress= ((res[0].T)).data + ((res[1])).data\n",
    "embedding = (ress/2)\n",
    "print(embedding.shape)\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2b12cd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.28826517,  0.12164679,  0.22372392,  0.07050588,  0.38010073],\n",
       "       [ 0.35405755, -0.08190916, -0.6919917 , -0.00300702,  0.01030795],\n",
       "       [ 0.64607024, -0.2998314 ,  0.27626485, -0.22294661, -0.80913925],\n",
       "       [-0.09001738, -0.30591652,  0.06811734, -0.13348739,  0.25930002],\n",
       "       [-0.39443386,  0.06893147,  0.27483577, -0.13503258,  0.38651553],\n",
       "       [-0.17295939,  0.00449817,  0.11369327,  0.21628049,  0.2731137 ],\n",
       "       [-0.2312125 , -0.07888495,  0.14975473, -0.06203181,  0.2909721 ],\n",
       "       [-0.28927362, -0.12738416,  0.16386008, -0.06982535,  0.26886347],\n",
       "       [-0.07405913, -0.2543118 , -0.12634334, -0.10032488, -0.07211134],\n",
       "       [-0.19814897, -0.2549076 ,  0.4619699 , -0.16948594,  0.3691212 ],\n",
       "       [ 0.15559053,  0.0780872 , -0.04998878, -0.14241827, -0.1518383 ],\n",
       "       [-0.28499806,  0.019077  ,  0.04508601, -0.23123997,  0.25785702],\n",
       "       [ 0.71647865, -0.24429075, -0.10465132, -0.10629469,  0.41394794],\n",
       "       [ 0.35334992,  0.08221771,  0.04917493, -0.09519197, -0.35864294],\n",
       "       [ 1.4646544 , -0.17177574, -0.1411833 ,  0.10546938,  0.4539159 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_np = embedding.numpy()\n",
    "embedding_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f20f66d",
   "metadata": {},
   "source": [
    "## Visualizaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7db032e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['he', 'is', 'a', 'king', 'she', 'queen', 'man', 'woman', 'warsaw',\n",
       "       'poland', 'capital', 'berlin', 'germany', 'paris', 'france'],\n",
       "      dtype='<U7')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_lable = np.array(list(word2idx.keys()))\n",
    "target_lable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6fcd9834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color = np.array(list(word2idx.values()))\n",
    "color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ff46823e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 5)\n",
      "(15,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/python-run/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/python-run/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:982: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class FeatureVisualize(object):\n",
    "    '''\n",
    "    Visualize features by TSNE\n",
    "    '''\n",
    "\n",
    "    def __init__(self, features, labels,color):\n",
    "        '''\n",
    "        features: (m,n)\n",
    "        labels: (m,)\n",
    "        '''\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.color = color\n",
    "\n",
    "    def plot_tsne(self, save_eps=False):\n",
    "        ''' Plot TSNE figure. Set save_eps=True if you want to save a .eps file.\n",
    "        '''\n",
    "        fig, axs = plt.subplots(figsize=(10,7))\n",
    "        tsne = TSNE(n_components=2, init='pca', random_state=0)\n",
    "        features = tsne.fit_transform(self.features)\n",
    "        x_min, x_max = np.min(features, 0), np.max(features, 0)\n",
    "        data = (features - x_min) / (x_max - x_min)\n",
    "        del features\n",
    "        for i in range(data.shape[0]):\n",
    "            plt.text(data[i, 0], data[i, 1], str(self.labels[i]),\n",
    "                     color=plt.cm.Set1(self.color[i] / 10.),\n",
    "                     fontdict={'weight': 'bold', 'size': 15})\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title('T-SNE')\n",
    "        if save_eps:\n",
    "            plt.savefig('tsne.eps', dpi=600, format='eps')\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    digits = datasets.load_digits(n_class=5)\n",
    "    features, labels = embedding_np,target_lable #digits.data, digits.target\n",
    "    print(features.shape)\n",
    "    print(labels.shape)\n",
    "    vis = FeatureVisualize(features, labels,color)\n",
    "    vis.plot_tsne(save_eps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b704261",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df87f75a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85727a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d08fb78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python-run]",
   "language": "python",
   "name": "conda-env-python-run-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "248.75px",
    "width": "237.76px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
